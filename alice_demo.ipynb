{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# **Text generation in the style of Alice's Adventures in Wonderland** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## **Import necessary dependencies** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## **Load and preprocess the data** ##\n",
    "First, we need to load and preprocess the data. The data is the full text of the book Alice's Adventures in Wonderland by Lewis Carroll. The book is on public domain and can be downloaded for free in multiple formats on Project Gutenberg website: https://www.gutenberg.org/wiki/Main_Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ufeffproject gutenberg's alice's adventures in wonderland, by lewis carroll\\n\\nthis ebook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.  you may copy it, give it away or\\nre-use it under the terms of the project gutenberg license included\\nwith this ebook or online at www.gutenberg.org\\n\\n\\ntitle: alice's adventures in wonderland\\n\\nauthor: lewis carroll\\n\\nposting date: june 25, 2008 [ebook #11]\\nrelease date: march, 1994\\n[last updated: december 20, 2011]\\n\\nlanguage: english\\n\\n\\n*** start of this project gutenberg ebook alice's adventures in wonderland ***\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nalice's adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down the rabbit-hole\\n\\nalice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought alice 'without pictur\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the text file and convert them all into lower case\n",
    "filename = \"data/wonderland.txt\"\n",
    "raw_text = open(filename, encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# print out the first 1000 characters\n",
    "raw_text[: 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "# get the list of all unique characters\n",
    "chars = sorted(list(set(raw_text)))\n",
    "\n",
    "# map between characters and their index, in both directions\n",
    "# so we can encode and decode between them\n",
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# print out all unique characters\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163781\n",
      "Total Vocab:  59\n"
     ]
    }
   ],
   "source": [
    "# number of raw characters and\n",
    "# number of unique characters in the text\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we split the book's text into subsequences of 100 continuous character, each subsequence will overlap with the previous one(and the next subsequence) 99 characters. In other words, we \"skip\" 1 character to make a new subsequence, so the learning algorithm can \"learn\" a character based on its 100 previous characters(which is its context).\n",
    "After that, we turn the characters into integers using the above look-up table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163681\n"
     ]
    }
   ],
   "source": [
    "# the number of characters in each subsequence\n",
    "seq_length  = 100\n",
    "# skip 1 character, meaning 2 continous subsequences\n",
    "# will have the same 99 characters\n",
    "skip = 1\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, skip):\n",
    "    # the context, which is the previous 100 characters\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    \n",
    "    # the correct character we want to predict\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    \n",
    "    # change them into indices\n",
    "    dataX.append([char2id[char] for char in seq_in])\n",
    "    dataY.append(char2id[seq_out])\n",
    "    \n",
    "# a pattern is a context to predict a character\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Because we choose our seq_length as 100 characters, the total number of patterns is less than 100 compared to the total number of characters(excluding the first 100 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reshape x to be [batch_size=n_patterns, timesteps=seq_length, input_dim=1] to \n",
    "# feed into LSTM network\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize the data\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## **Model training** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can build our deep learning model. It has a single  LSTM layer consisting of 256 memory units and a dropout fraction of 20% to prevent overfitting. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 59 characters.\n",
    "\n",
    "The model is compiled using the log loss cross-entropy function as the loss function, and the adam algorithm as the optimizier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# input LSTM layer\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "# dropout layer\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# save the model after each epoch if there is an improvement\n",
    "# (the loss reduced) for easily retrain and reload\n",
    "filepath=\"model_checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's time to train the model. This step is very expensive computationaly, in my computer with a pretty powerful NVIDIA GPU card, it took at least 8 minutes for each epoch, hence a training session of 10 epochs costed more than 80 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 00000: loss improved from inf to 2.98379, saving model to model_checkpoints/weights-improvement-00-2.9838.hdf5\n",
      "513s - loss: 2.9838\n",
      "Epoch 2/10\n",
      "Epoch 00001: loss improved from 2.98379 to 2.80069, saving model to model_checkpoints/weights-improvement-01-2.8007.hdf5\n",
      "474s - loss: 2.8007\n",
      "Epoch 3/10\n",
      "Epoch 00002: loss improved from 2.80069 to 2.71612, saving model to model_checkpoints/weights-improvement-02-2.7161.hdf5\n",
      "458s - loss: 2.7161\n",
      "Epoch 4/10\n",
      "Epoch 00003: loss improved from 2.71612 to 2.64447, saving model to model_checkpoints/weights-improvement-03-2.6445.hdf5\n",
      "447s - loss: 2.6445\n",
      "Epoch 5/10\n",
      "Epoch 00004: loss improved from 2.64447 to 2.59306, saving model to model_checkpoints/weights-improvement-04-2.5931.hdf5\n",
      "452s - loss: 2.5931\n",
      "Epoch 6/10\n",
      "Epoch 00005: loss improved from 2.59306 to 2.54385, saving model to model_checkpoints/weights-improvement-05-2.5439.hdf5\n",
      "431s - loss: 2.5439\n",
      "Epoch 7/10\n",
      "Epoch 00006: loss improved from 2.54385 to 2.49083, saving model to model_checkpoints/weights-improvement-06-2.4908.hdf5\n",
      "435s - loss: 2.4908\n",
      "Epoch 8/10\n",
      "Epoch 00007: loss improved from 2.49083 to 2.44585, saving model to model_checkpoints/weights-improvement-07-2.4458.hdf5\n",
      "460s - loss: 2.4458\n",
      "Epoch 9/10\n",
      "Epoch 00008: loss improved from 2.44585 to 2.40147, saving model to model_checkpoints/weights-improvement-08-2.4015.hdf5\n",
      "496s - loss: 2.4015\n",
      "Epoch 10/10\n",
      "Epoch 00009: loss improved from 2.40147 to 2.36386, saving model to model_checkpoints/weights-improvement-09-2.3639.hdf5\n",
      "504s - loss: 2.3639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9f38a81a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model into the data\n",
    "model.fit(X, y, epochs=10, batch_size=128, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that the loss did reduced after each epoch, so the last epoch also provided the best weight values. We will stop here and try to generate some text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## **Using LSTM network to generate text** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load the network weights from the best epoch\n",
    "filename = \"model_checkpoints/weights-improvement-09-2.3639.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To generate new text, we can randomly select a pattern(which is a collection of 100 characters) in the dataset, use this pattern as the context for the learning algorithm to predict the first next most likely character. Then the next pattern will have the same 99 characters as the previous pattern plus the newly predicted character, from here the algorithm will predict the second character. And so on. We will generate 500 characters and then convert them from integer indices back into human-readable letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context:\n",
      "\" do such a\n",
      "thing before, and he wasn't going to begin at his time of life.\n",
      "\n",
      "the king's argument was,  \"\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "Generated text: \n",
      " \"\n",
      " she soeen sai so the toree the was so tee to the three sare the was so tee to the three hare th the the wooee sas in the care an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade an the cate an the cade a\n",
      "\" \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random pattern \n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "# print out the pattern in human-readable form\n",
    "print (\"The context:\")\n",
    "print (\"\\\"\", ''.join([id2char[index] for index in pattern]), \"\\\"\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"\\nGenerated text: \\n\", \"\\\"\")\n",
    "\n",
    "\n",
    "# AI writer in action!\n",
    "for i in range(500):\n",
    "    # reshape x to be [batch_size=1, timesteps=seq_length, input_dim=1] to \n",
    "    # feed into LSTM network\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    \n",
    "    # normalize the data\n",
    "    x = x / float(n_vocab)\n",
    "    \n",
    "    # predict the next character from its context x\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    \n",
    "    # get the index of the predicted character from sotfmax function\n",
    "    index = numpy.argmax(prediction)\n",
    "    \n",
    "    # change index into letter\n",
    "    result = id2char[index]\n",
    "    \n",
    "    # \n",
    "    seq_in = [id2char[index] for index in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\n\\\"\", \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## **Conclusion** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Well, right now the result looks very bad, they are all nonsense(well, just like the original Alice's Adventures in Wonderland anyway!). This is just a very simple model though, and it can be improved by:\n",
    "- Building a more complex neural network, for example, add another LSTM layer, or just add more memory units in the layer.\n",
    "- Training in more epoch(10 is very small but already very costly computational speaking) and larger batch size(require more memory)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
